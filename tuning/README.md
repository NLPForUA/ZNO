# Empowering Smaller Models: Tuning LLaMA and Gemma with Chain-of-Thought for Ukrainian Exam Tasks

[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/NLPForUA/ZNO/blob/main/LICENSE)
[![Data License](https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg)](https://github.com/NLPForUA/ZNO/blob/main/DATA_LICENSE)
[![arXiv](https://img.shields.io/badge/arXiv-2503.13988-blue.svg?logo=arxiv&logoColor=white.svg)](https://arxiv.org/abs/2503.13988)

This section contains code and notebooks for Gemma 2 and LLaMA 3.1-3.2 models PEFT tuning on Ukrainian language exam tasks (ZNO and NMT) with and without chain-of-thought output.

## Updates:
- 2025-03-18: LLaMA models release on HF: to be added 19/03/2025
- 2025-03-19: [arXiv preprint](https://arxiv.org/abs/2503.13988)
- 2025-03-18: [Gemma models release on HF](https://huggingface.co/NLPForUA)

### Citation

Please cite this paper if you use hf models, data or code in this folder.

```
@article{syromiatnikov2025empoweringsmallermodelstuning,
 title={Empowering Smaller Models: Tuning LLaMA and Gemma with Chain-of-Thought for Ukrainian Exam Tasks},
 url={https://arxiv.org/abs/2503.13988},
 DOI={doi.org/10.48550/arXiv.2503.13988},
 number={1},
 journal={},
 author={Mykyta Syromiatnikov and Victoria Ruvinskaya and Nataliia Komleva},
 year={2025},
 month={March}, 
 pages={}
}
```